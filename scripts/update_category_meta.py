"""
Update category README statistics blocks based on the latest crawl results.
"""

from __future__ import annotations

import sys
from pathlib import Path
from typing import Dict, Iterable, List

SCRIPT_DIR = Path(__file__).resolve().parent
ROOT_DIR = SCRIPT_DIR.parent
if str(ROOT_DIR) not in sys.path:
    sys.path.insert(0, str(ROOT_DIR))

from config import ARTICLE_CATEGORIES, DOCS_DIR, ensure_structure
from scripts.generate_stats import (
    generate_stats,
    list_markdown_files,
    get_last_update_from_files,
)

STATS_START = "<!-- stats:start -->"
STATS_END = "<!-- stats:end -->"
ARTICLES_START = "<!-- articles:start -->"
ARTICLES_END = "<!-- articles:end -->"


def render_stats_block(total: int, last_update: str) -> str:
    return "\n".join(
        [
            STATS_START,
            "",
            f"- **æ€»è®¡**: {total} ç¯‡ç ”æŠ¥",
            f"- **æœ€åæ›´æ–°**: {last_update}",
            "",
            STATS_END,
        ]
    )


def replace_block(content: str, block: str) -> str:
    if STATS_START in content and STATS_END in content:
        start = content.index(STATS_START)
        end = content.index(STATS_END) + len(STATS_END)
        return content[:start] + block + content[end:]
    # Append if markers do not exist yet.
    if content.endswith("\n"):
        return content + "\n" + block + "\n"
    return content + "\n\n" + block + "\n"


def render_articles_block(files: Iterable[Path], category_name: str) -> str:
    """ç”Ÿæˆæ–‡ç« åˆ—è¡¨åŒºå—ï¼Œå¸¦æ¸…æ™°çš„æ ‡é¢˜å’Œç»“æ„"""
    from urllib.parse import quote
    
    lines = [ARTICLES_START, "", "## ğŸ“„ æ–‡ç« åˆ—è¡¨", ""]
    
    if not files:
        lines.append("> æš‚æ— å†…å®¹ï¼Œç¨åå†æ¥çœ‹çœ‹å§ã€‚")
        lines.append("")
        lines.append(ARTICLES_END)
        return "\n".join(lines)
    
    # æŒ‰æ—¥æœŸåˆ†ç»„å±•ç¤ºï¼ˆä»æ–°åˆ°æ—§ï¼‰
    for idx, file_path in enumerate(files, 1):
        title = file_path.stem
        
        # æå–æ—¥æœŸå’Œæ ‡é¢˜
        if len(title) > 11 and title[4] == "." and title[7] == "." and title[10] == "-":
            date_part = title[:10]
            title_part = title[11:]
        else:
            date_part = ""
            title_part = title
        
        # æ„å»ºç›¸å¯¹è·¯å¾„ï¼šåˆ†ç±»ç›®å½•/æ–‡ä»¶åï¼ˆå¸¦ URL ç¼–ç ï¼‰
        # ä¾‹å¦‚: å…¨éƒ¨ç ”æŠ¥/2025.10.27-xxx.md
        rel_path = f"{category_name}/{quote(file_path.name)}"
        
        # æ ¼å¼åŒ–è¾“å‡º
        if date_part:
            lines.append(f"{idx}. **[{title_part}]({rel_path})** - `{date_part}`")
        else:
            lines.append(f"{idx}. **[{title_part}]({rel_path})**")
    
    lines.append("")
    lines.append(f"> å…± {len(list(files))} ç¯‡ç ”æŠ¥")
    lines.append("")
    lines.append(ARTICLES_END)
    return "\n".join(lines)


def replace_articles_block(content: str, block: str) -> str:
    if ARTICLES_START in content and ARTICLES_END in content:
        start = content.index(ARTICLES_START)
        end = content.index(ARTICLES_END) + len(ARTICLES_END)
        return content[:start] + block + content[end:]
    if content.endswith("\n"):
        return content + block + "\n"
    return content + "\n" + block + "\n"


def update_category_readme(stats: Dict[str, object]) -> None:
    import re
    
    category_counts: Dict[str, int] = stats.get("categories", {})

    # æå–æ–‡ç«  ID çš„è¾…åŠ©å‡½æ•°
    def extract_article_id(file_path: Path) -> int:
        try:
            content = file_path.read_text(encoding='utf-8')
            match = re.search(r'æ–‡ç« ID[ï¼š:]\s*(\d+)', content)
            if match:
                return int(match.group(1))
        except:
            pass
        return -1  # æ²¡æœ‰ ID çš„æ’æœ€å

    for category, path in ARTICLE_CATEGORIES.items():
        readme_path = path / "README.md"
        if not readme_path.exists():
            continue

        files: List[Path] = sorted(
            list_markdown_files(path),
            key=extract_article_id,  # æ”¹ä¸ºæŒ‰æ–‡ç«  ID æ’åº
            reverse=True,
        )
        last_update = (
            stats["last_update"]
            if category == "å…¨éƒ¨ç ”æŠ¥"
            else get_last_update_from_files(files)
        )
        content = readme_path.read_text(encoding="utf-8")
        block = render_stats_block(category_counts.get(category, 0), last_update)
        content = replace_block(content, block)
        articles_block = render_articles_block(files, category)
        content = replace_articles_block(content, articles_block)
        readme_path.write_text(content, encoding="utf-8")
        
        print(f"âœ… æ›´æ–° {category} README: {len(list(files))} ç¯‡æ–‡ç« ")


def update_homepage(stats: Dict[str, object]) -> None:
    """æ›´æ–° HOME.md çš„ç»Ÿè®¡å—å’Œæœ€æ–°æ”¶å½•åˆ—è¡¨"""
    from urllib.parse import quote
    import re
    
    homepage = DOCS_DIR / "HOME.md"
    if not homepage.exists():
        return

    content = homepage.read_text(encoding="utf-8")
    
    # æ›´æ–°ç»Ÿè®¡å—
    stats_block = render_stats_block(stats["total_articles"], stats["last_update"])
    content = replace_block(content, stats_block)
    
    # æ›´æ–°æœ€æ–°æ”¶å½•åˆ—è¡¨
    all_files = list(list_markdown_files(ARTICLE_CATEGORIES["å…¨éƒ¨ç ”æŠ¥"]))
    
    def extract_article_id(file_path: Path) -> int:
        try:
            file_content = file_path.read_text(encoding='utf-8')
            match = re.search(r'æ–‡ç« ID[ï¼š:]\s*(\d+)', file_content)
            if match:
                return int(match.group(1))
        except:
            pass
        return -1
    
    # è·å–æœ€æ–° 8 ç¯‡æ–‡ç« 
    recent_files = sorted(all_files, key=extract_article_id, reverse=True)[:8]
    
    latest_lines = [
        "### ğŸ†• æœ€æ–°æ”¶å½•",
        "",
        "<!-- latest:start -->",
        "",
        "ä»¥ä¸‹æ˜¯æœ€è¿‘æ”¶å½•çš„ 8 ç¯‡ç ”æŠ¥ï¼ˆæŒ‰æ–‡ç«  ID é™åºæ’åˆ—ï¼‰ï¼š",
        "",
    ]
    
    for file_path in recent_files:
        # ä½¿ç”¨ URL ç¼–ç çš„ç›¸å¯¹è·¯å¾„
        rel_path = f"/å…¨éƒ¨ç ”æŠ¥/{quote(file_path.name)}"
        # æå–æ ‡é¢˜ï¼ˆå»æ‰æ—¥æœŸå‰ç¼€ï¼‰
        title = file_path.stem
        article_id = extract_article_id(file_path)
        if article_id > 0:
            latest_lines.append(f"- [{title}]({rel_path}) - **æ–‡ç« ID: {article_id}**")
        else:
            latest_lines.append(f"- [{title}]({rel_path})")
    
    latest_lines.extend([
        "",
        f"[æŸ¥çœ‹æ‰€æœ‰ {stats['total_articles']} ç¯‡ç ”æŠ¥ â†’](/å…¨éƒ¨ç ”æŠ¥/)",
        "",
        "<!-- latest:end -->",
    ])
    
    latest_block = "\n".join(latest_lines)
    
    # æ›¿æ¢æœ€æ–°æ”¶å½•å—
    LATEST_START = "<!-- latest:start -->"
    LATEST_END = "<!-- latest:end -->"
    
    if LATEST_START in content and LATEST_END in content:
        start = content.index(LATEST_START)
        # æ‰¾åˆ°å‰é¢çš„æ ‡é¢˜è¡Œ
        title_start = content.rfind("### ğŸ†• æœ€æ–°æ”¶å½•", 0, start)
        if title_start != -1:
            start = title_start
        end = content.index(LATEST_END) + len(LATEST_END)
        content = content[:start] + latest_block + content[end:]
    
    homepage.write_text(content, encoding="utf-8")
    print("âœ… æ›´æ–° HOME.md (ç»Ÿè®¡ + æœ€æ–°æ”¶å½•)")


def generate_index_page(stats: Dict[str, object]) -> None:
    from urllib.parse import quote
    import re
    
    index_path = DOCS_DIR / "index.md"
    
    # è·å–æ‰€æœ‰æ–‡ç« 
    all_files = list(list_markdown_files(ARTICLE_CATEGORIES["å…¨éƒ¨ç ”æŠ¥"]))
    
    # æå–æ¯ä¸ªæ–‡ä»¶çš„ ID
    def extract_article_id(file_path: Path) -> int:
        try:
            content = file_path.read_text(encoding='utf-8')
            # ä»æ–‡ä»¶å†…å®¹ä¸­æå– "æ–‡ç« ID: 686" è¿™æ ·çš„è¡Œ
            match = re.search(r'æ–‡ç« ID[ï¼š:]\s*(\d+)', content)
            if match:
                return int(match.group(1))
        except:
            pass
        return -1  # æ²¡æœ‰ ID çš„æ’æœ€å
    
    # æŒ‰ ID å€’åºæ’åºï¼ˆID è¶Šå¤§è¶Šæ–°ï¼‰
    recent_files = sorted(
        all_files,
        key=extract_article_id,
        reverse=True,
    )[:8]

    recent_lines = []
    for file_path in recent_files:
        # ç›¸å¯¹è·¯å¾„æ ¼å¼: å…¨éƒ¨ç ”æŠ¥/æ–‡ä»¶å.md (å¸¦ URL ç¼–ç )
        rel_path = f"å…¨éƒ¨ç ”æŠ¥/{quote(file_path.name)}"
        title = file_path.stem
        article_id = extract_article_id(file_path)
        # æ˜¾ç¤º ID æ–¹ä¾¿è°ƒè¯•
        if article_id > 0:
            recent_lines.append(f"- [{title}]({rel_path}) `#{article_id}`")
        else:
            recent_lines.append(f"- [{title}]({rel_path})")

    stats_block = render_stats_block(stats["total_articles"], stats["last_update"])

    content = "\n".join(
        [
            "# ğŸŠ é³„é±¼æ´¾æŠ•èµ„ç ”æŠ¥æŒ‡æ•°ç«™",
            "",
            "> æ¯æ—¥è‡ªåŠ¨æ”¶å½•æœ€æ–°ç ”æŠ¥ï¼Œå®è§‚ä¸è¡Œä¸šåŠ¨å‘ä¸€ç«™æŒæ¡",
            "",
            stats_block,
            "",
            "## ğŸ“š å¿«é€Ÿå¯¼èˆª",
            "- [ğŸ“‘ å…¨éƒ¨ç ”æŠ¥](/å…¨éƒ¨ç ”æŠ¥/)",
            "- [ğŸ“ˆ å®è§‚åˆ†æ](/å®è§‚åˆ†æ/)",
            "- [ğŸ­ è¡Œä¸šåˆ†æ](/è¡Œä¸šåˆ†æ/)",
            "",
            "## ğŸ†• æœ€æ–°æ”¶å½•",
        ]
        + (recent_lines or ["- æš‚æ— æœ€æ–°å†…å®¹ï¼Œç¨åå†æ¥çœ‹çœ‹å§ã€‚"])
        + [
            "",
            "## ğŸš€ ä½¿ç”¨å»ºè®®",
            "- ä½¿ç”¨å·¦ä¾§ä¾§è¾¹æ å¿«é€Ÿåˆ‡æ¢åˆ†ç±»",
            "- åˆ©ç”¨æœç´¢æ¡†æŒ‰å…³é”®è¯å®šä½ç ”æŠ¥",
            "- æ¯ç¯‡æ–‡ç« é¡¶éƒ¨ä¼šæ ‡æ³¨æ›´æ–°æ—¥æœŸå’Œé˜…è¯»å­—æ•°",
        ]
    )

    index_path.write_text(content + "\n", encoding="utf-8")


def main() -> None:
    ensure_structure()
    stats = generate_stats()
    update_category_readme(stats)
    update_homepage(stats)
    # generate_index_page(stats)  # ä¸å†éœ€è¦ index.mdï¼ŒHOME.md å·²åŒ…å«æœ€æ–°æ”¶å½•


if __name__ == "__main__":
    main()
