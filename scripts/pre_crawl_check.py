"""
çˆ¬è™«å‰ç½®æ£€æŸ¥å’Œä¿®å¤è„šæœ¬
ç¡®ä¿ index.json ç»“æ„å®Œæ•´ï¼Œè‡ªåŠ¨ä¿®å¤å¸¸è§é—®é¢˜
ç”¨äº GitHub Actions è‡ªåŠ¨åŒ–æµç¨‹
"""

from __future__ import annotations

import json
import sys
from pathlib import Path
from datetime import datetime

SCRIPT_DIR = Path(__file__).resolve().parent
ROOT_DIR = SCRIPT_DIR.parent
if str(ROOT_DIR) not in sys.path:
    sys.path.insert(0, str(ROOT_DIR))

from config import INDEX_FILE, ARTICLE_CATEGORIES


def ensure_index_structure(data: dict) -> tuple[dict, bool]:
    """ç¡®ä¿ index.json æœ‰æ‰€æœ‰å¿…éœ€çš„å­—æ®µ"""
    modified = False
    required_fields = {
        "saved_ids": [],
        "downloaded_ids": [],
        "missing_ids": [],
        "pending_ids": [],
        "last_probed_id": 0,
        "next_probe_id": 1,
        "probe_history": [],
    }
    
    for field, default_value in required_fields.items():
        if field not in data:
            data[field] = default_value
            modified = True
            print(f"   ğŸ”§ æ·»åŠ ç¼ºå¤±å­—æ®µ: {field}")
    
    return data, modified


def sync_downloaded_with_files(data: dict) -> tuple[dict, bool]:
    """åŒæ­¥ downloaded_ids ä¸å®é™…æ–‡ä»¶"""
    modified = False
    
    # æ”¶é›†æ‰€æœ‰å®é™…å­˜åœ¨çš„æ–‡ä»¶åï¼ˆå»é‡ï¼‰
    existing_files = set()
    for category, path in ARTICLE_CATEGORIES.items():
        if path.exists():
            for file in path.iterdir():
                if file.suffix == ".md" and file.name.lower() != "readme.md":
                    existing_files.add(file.stem)
    
    saved_ids = set(data.get("saved_ids", []))
    downloaded_ids = set(data.get("downloaded_ids", []))
    
    # å¦‚æœ downloaded_ids ä¸ºç©ºï¼Œä½†æœ‰å·²ä¿å­˜çš„ IDï¼Œåˆå§‹åŒ–å®ƒ
    if not downloaded_ids and saved_ids:
        print(f"   ğŸ”§ åˆå§‹åŒ– downloaded_ids: {len(saved_ids)} ç¯‡")
        data["downloaded_ids"] = sorted(list(saved_ids))
        modified = True
    
    # æ£€æŸ¥ saved_ids å’Œ downloaded_ids çš„ä¸€è‡´æ€§
    not_downloaded = saved_ids - downloaded_ids
    if not_downloaded and len(not_downloaded) == len(saved_ids):
        # æ‰€æœ‰å·²æ¢æµ‹çš„éƒ½æœªæ ‡è®°ä¸ºå·²ä¸‹è½½ï¼Œè‡ªåŠ¨åŒæ­¥
        print(f"   ğŸ”§ åŒæ­¥ downloaded_ids: {len(saved_ids)} ç¯‡")
        data["downloaded_ids"] = sorted(list(saved_ids))
        modified = True
    
    return data, modified


def validate_and_clean(data: dict) -> tuple[dict, bool]:
    """éªŒè¯å’Œæ¸…ç†æ•°æ®"""
    modified = False
    
    # ç¡®ä¿æ‰€æœ‰ ID åˆ—è¡¨éƒ½æ˜¯æ’åºçš„
    for field in ["saved_ids", "downloaded_ids", "missing_ids", "pending_ids"]:
        if field in data and isinstance(data[field], list):
            original = data[field]
            cleaned = sorted(set(int(x) for x in original))
            if cleaned != original:
                data[field] = cleaned
                modified = True
                print(f"   ğŸ”§ æ¸…ç†å¹¶æ’åº {field}")
    
    # éªŒè¯ probe_history é•¿åº¦
    if len(data.get("probe_history", [])) > 50:
        data["probe_history"] = data["probe_history"][-20:]
        modified = True
        print("   ğŸ”§ æ¸…ç†è¿‡é•¿çš„ probe_history")
    
    # éªŒè¯ next_probe_id çš„åˆç†æ€§
    saved_ids = data.get("saved_ids", [])
    if saved_ids:
        max_id = max(saved_ids)
        next_probe = data.get("next_probe_id", 1)
        if next_probe < max_id:
            data["next_probe_id"] = max_id + 1
            modified = True
            print(f"   ğŸ”§ ä¿®æ­£ next_probe_id: {max_id + 1}")
    
    return data, modified


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” å¼€å§‹å‰ç½®æ£€æŸ¥...\n")
    
    # 1. æ£€æŸ¥ index.json æ˜¯å¦å­˜åœ¨
    if not INDEX_FILE.exists():
        print("âš ï¸  index.json ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°æ–‡ä»¶")
        data = {
            "saved_ids": [],
            "downloaded_ids": [],
            "missing_ids": [],
            "pending_ids": [],
            "last_probed_id": 0,
            "next_probe_id": 1,
            "probe_history": [],
        }
        INDEX_FILE.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")
        print("âœ… å·²åˆ›å»ºé»˜è®¤ index.json\n")
        return 0
    
    # 2. è¯»å–å¹¶éªŒè¯
    try:
        data = json.loads(INDEX_FILE.read_text(encoding="utf-8"))
    except json.JSONDecodeError as e:
        print(f"âŒ index.json æ ¼å¼é”™è¯¯: {e}")
        print("   å°è¯•å¤‡ä»½å¹¶é‡æ–°åˆ›å»º...")
        backup = INDEX_FILE.with_suffix(".json.backup")
        INDEX_FILE.rename(backup)
        print(f"   å¤‡ä»½ä¿å­˜è‡³: {backup}")
        return 1
    
    print("ğŸ“‹ å½“å‰çŠ¶æ€:")
    print(f"   å·²æ¢æµ‹: {len(data.get('saved_ids', []))} ç¯‡")
    print(f"   å·²ä¸‹è½½: {len(data.get('downloaded_ids', []))} ç¯‡")
    print(f"   ç¼ºå¤±è®°å½•: {len(data.get('missing_ids', []))} ä¸ª")
    print(f"   å¾…ä¸‹è½½: {len(data.get('pending_ids', []))} ä¸ª")
    print(f"   ä¸‹æ¬¡æ¢æµ‹èµ·ç‚¹: ID {data.get('next_probe_id', 1)}\n")
    
    # 3. æ‰§è¡Œæ£€æŸ¥å’Œä¿®å¤
    total_modified = False
    
    print("ğŸ”§ æ‰§è¡Œæ£€æŸ¥å’Œä¿®å¤:")
    data, mod1 = ensure_index_structure(data)
    data, mod2 = sync_downloaded_with_files(data)
    data, mod3 = validate_and_clean(data)
    
    total_modified = mod1 or mod2 or mod3
    
    if not total_modified:
        print("   âœ… æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼Œæ— éœ€ä¿®å¤")
    
    # 4. ä¿å­˜ä¿®æ”¹
    if total_modified:
        INDEX_FILE.write_text(
            json.dumps(data, ensure_ascii=False, indent=2),
            encoding="utf-8"
        )
        print("\nğŸ’¾ å·²ä¿å­˜ä¿®å¤åçš„ index.json")
    
    # 5. æœ€ç»ˆæŠ¥å‘Š
    print("\n" + "=" * 60)
    print("âœ… å‰ç½®æ£€æŸ¥å®Œæˆ")
    print(f"   å·²æ¢æµ‹: {len(data.get('saved_ids', []))} ç¯‡")
    print(f"   å·²ä¸‹è½½: {len(data.get('downloaded_ids', []))} ç¯‡")
    print(f"   å‡†å¤‡å¼€å§‹çˆ¬å–...")
    print("=" * 60 + "\n")
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
