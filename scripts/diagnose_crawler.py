"""
çˆ¬è™«è¯Šæ–­å’Œä¿®å¤å·¥å…·
æ£€æŸ¥ index.json çš„å®Œæ•´æ€§å¹¶æä¾›ä¿®å¤å»ºè®®
"""

from __future__ import annotations

import json
import sys
from pathlib import Path
from datetime import datetime

SCRIPT_DIR = Path(__file__).resolve().parent
ROOT_DIR = SCRIPT_DIR.parent
if str(ROOT_DIR) not in sys.path:
    sys.path.insert(0, str(ROOT_DIR))

from config import INDEX_FILE, ARTICLE_CATEGORIES


def count_actual_files() -> dict[str, int]:
    """ç»Ÿè®¡å®é™…å­˜åœ¨çš„æ–‡ä»¶æ•°é‡ï¼ˆå»é‡ï¼‰"""
    counts = {}
    all_files = set()  # ç”¨äºå»é‡
    
    for category, path in ARTICLE_CATEGORIES.items():
        if path.exists():
            files = [
                f.name for f in path.iterdir()
                if f.suffix == ".md" and f.name.lower() != "readme.md"
            ]
            counts[category] = len(files)
            all_files.update(files)
        else:
            counts[category] = 0
    
    # å»é‡åçš„æ€»æ•°
    counts["total"] = len(all_files)
    counts["total_with_duplicates"] = sum(counts[k] for k in counts if k != "total")
    return counts


def diagnose():
    """è¯Šæ–­çˆ¬è™«çŠ¶æ€"""
    print("ğŸ” å¼€å§‹è¯Šæ–­çˆ¬è™«çŠ¶æ€...\n")
    
    # 1. æ£€æŸ¥ index.json
    if not INDEX_FILE.exists():
        print("âŒ index.json æ–‡ä»¶ä¸å­˜åœ¨ï¼")
        return
    
    with open(INDEX_FILE, encoding="utf-8") as f:
        index_data = json.load(f)
    
    saved_ids = set(index_data.get("saved_ids", []))
    downloaded_ids = set(index_data.get("downloaded_ids", []))
    missing_ids = set(index_data.get("missing_ids", []))
    pending_ids = set(index_data.get("pending_ids", []))
    
    print("ğŸ“Š Index.json çŠ¶æ€:")
    print(f"   å·²æ¢æµ‹æ–‡ç« : {len(saved_ids)} ç¯‡")
    print(f"   å·²ä¸‹è½½æ–‡ç« : {len(downloaded_ids)} ç¯‡")
    print(f"   ç¼ºå¤±è®°å½•: {len(missing_ids)} ä¸ª")
    print(f"   å¾…ä¸‹è½½: {len(pending_ids)} ä¸ª")
    print(f"   ä¸Šæ¬¡æ¢æµ‹ ID: {index_data.get('last_probed_id', 0)}")
    print(f"   ä¸‹æ¬¡æ¢æµ‹èµ·ç‚¹: {index_data.get('next_probe_id', 1)}")
    
    if saved_ids:
        print(f"   ID èŒƒå›´: {min(saved_ids)} - {max(saved_ids)}")
    
    # 2. ç»Ÿè®¡å®é™…æ–‡ä»¶
    print("\nğŸ“ å®é™…æ–‡ä»¶ç»Ÿè®¡:")
    file_counts = count_actual_files()
    for category, count in file_counts.items():
        if category not in ("total", "total_with_duplicates"):
            print(f"   {category}: {count} ç¯‡")
    print(f"   å„åˆ†ç±»æ€»è®¡: {file_counts['total_with_duplicates']} ç¯‡ (å«é‡å¤)")
    print(f"   å»é‡åæ€»è®¡: {file_counts['total']} ç¯‡")
    
    unique_count = file_counts['total']
    
    # 3. åˆ†æé—®é¢˜
    print("\nğŸ” é—®é¢˜åˆ†æ:")
    
    # é—®é¢˜1: å·²æ¢æµ‹ä½†æœªä¸‹è½½
    not_downloaded = saved_ids - downloaded_ids
    if not_downloaded:
        print(f"   âš ï¸  æœ‰ {len(not_downloaded)} ç¯‡å·²æ¢æµ‹ä½†æœªä¸‹è½½")
        print(f"      ID: {sorted(list(not_downloaded))[:20]}{'...' if len(not_downloaded) > 20 else ''}")
    else:
        print("   âœ… æ‰€æœ‰å·²æ¢æµ‹çš„æ–‡ç« éƒ½å·²ä¸‹è½½")
    
    # é—®é¢˜2: ä¸‹è½½æ•°é‡ä¸åŒ¹é…
    if len(downloaded_ids) != unique_count:
        diff = unique_count - len(downloaded_ids)
        if diff > 0:
            print(f"   âš ï¸  å®é™…æ–‡ä»¶æ¯”ä¸‹è½½è®°å½•å¤š {diff} ç¯‡")
            print(f"      å¯èƒ½åŸå› : åŒä¸€æ–‡ç« ä¿å­˜åˆ°å¤šä¸ªåˆ†ç±»ï¼Œæˆ–æ‰‹åŠ¨æ·»åŠ äº†æ–‡ä»¶")
        else:
            print(f"   âš ï¸  ä¸‹è½½è®°å½•æ¯”å®é™…æ–‡ä»¶å¤š {-diff} ç¯‡ï¼ˆæ–‡ä»¶å¯èƒ½è¢«åˆ é™¤ï¼‰")
    else:
        print("   âœ… ä¸‹è½½è®°å½•ä¸å»é‡åæ–‡ä»¶æ•°é‡ä¸€è‡´")
    
    # é—®é¢˜3: ID é—´éš™
    if saved_ids:
        max_id = max(saved_ids)
        expected_count = max_id - len(missing_ids)
        actual_count = len(saved_ids)
        gap = expected_count - actual_count
        
        if gap > 50:
            print(f"   âš ï¸  ID èŒƒå›´å†…æœ‰å¤§é‡é—´éš™ (çº¦ {gap} ä¸ª)")
            print(f"      å»ºè®®è¿è¡Œæ‰«ææ¨¡å¼å¡«è¡¥é—´éš™")
    
    # é—®é¢˜4: å¾…ä¸‹è½½é˜Ÿåˆ—
    if pending_ids:
        print(f"   âš ï¸  æœ‰ {len(pending_ids)} ç¯‡æ–‡ç« åœ¨å¾…ä¸‹è½½é˜Ÿåˆ—")
        print(f"      å»ºè®®è¿è¡Œçˆ¬è™«å®Œæˆä¸‹è½½")
    else:
        print("   âœ… å¾…ä¸‹è½½é˜Ÿåˆ—ä¸ºç©º")
    
    # 4. å»ºè®®
    print("\nğŸ’¡ å»ºè®®æ“ä½œ:")
    
    if not_downloaded:
        print("   1. åˆå§‹åŒ– downloaded_ids (å¦‚æœè¿™äº›æ–‡ç« ç¡®å®å·²å­˜åœ¨):")
        print("      python scripts/fix_index.py --init-downloaded")
    
    if pending_ids:
        print("   2. å®Œæˆå¾…ä¸‹è½½é˜Ÿåˆ—:")
        print("      python crawler/fetch_reports.py")
    
    next_probe = index_data.get('next_probe_id', 1)
    if next_probe < 1000:  # å‡è®¾åº”è¯¥æœ‰æ›´å¤šæ–‡ç« 
        print(f"   3. ç»§ç»­æ¢æµ‹æ–°æ–‡ç«  (å½“å‰å‡†å¤‡ä» ID {next_probe} å¼€å§‹):")
        print("      python crawler/fetch_reports.py --batch-size 100")
    
    if len(saved_ids) < 500:  # å¦‚æœæ€»æ•°å°‘äºé¢„æœŸ
        print("   4. è€ƒè™‘æ‰«æå†å²åŒºé—´å¡«è¡¥ç¼ºå¤±:")
        print("      python crawler/fetch_reports.py --start-id 1 --end-id 500")
    
    print("\n" + "=" * 60)
    print(f"è¯Šæ–­å®Œæˆ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")


if __name__ == "__main__":
    diagnose()
