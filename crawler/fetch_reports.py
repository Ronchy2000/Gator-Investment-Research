"""
é³„é±¼æ´¾ç ”æŠ¥çˆ¬è™« - å†…å®¹ä¸‹è½½å™¨

âš ï¸ æž¶æž„è¯´æ˜Ž (2025-11-01 é‡æž„):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
æœ¬è„šæœ¬ä¸“æ³¨äºŽä¸‹è½½æ–‡ç« å†…å®¹,ä¸å†è´Ÿè´£è¾¹ç•ŒæŽ¢æµ‹ã€‚

èŒè´£åˆ†å·¥:
1. scripts/pre_crawl_check.py  â†’ è¾¹ç•ŒæŽ¢æµ‹ (è½»é‡çº§,å¿«é€Ÿ)
2. crawler/fetch_reports.py    â†’ å†…å®¹ä¸‹è½½ (é‡é‡çº§,å®Œæ•´)

å·¥ä½œæµç¨‹:
1. å…ˆè¿è¡Œ pre_crawl_check.py æŽ¢æµ‹è¾¹ç•Œ,å†™å…¥ last_probed_id
2. å†è¿è¡Œ fetch_reports.py ä¸‹è½½æ–‡ç« ,è¯»å– last_probed_id ä½œä¸ºè¾¹ç•Œ
3. è‡ªåŠ¨è·³è¿‡å·²ä¸‹è½½çš„æ–‡ç« ,å®žçŽ°å¢žé‡æ›´æ–°

ä½¿ç”¨æ–¹æ³•:
# å¢žé‡ä¸‹è½½ (æŽ¨è)
python crawler/fetch_reports.py

# é™åˆ¶å•æ¬¡ä¸‹è½½æ•°é‡
python crawler/fetch_reports.py --max-requests 100

# æ‰‹åŠ¨ä¸‹è½½æŒ‡å®šèŒƒå›´
python crawler/fetch_reports.py --start-id 400 --end-id 500
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""

from __future__ import annotations

import argparse
import json
import re
import sys
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple
from urllib.parse import urljoin

from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.common.exceptions import (
    NoSuchElementException,
    TimeoutException,
    WebDriverException,
)
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait

SCRIPT_DIR = Path(__file__).resolve().parent
ROOT_DIR = SCRIPT_DIR.parent
if str(ROOT_DIR) not in sys.path:
    sys.path.insert(0, str(ROOT_DIR))

from config import ARTICLE_CATEGORIES, INDEX_FILE, RAW_HTML_DIR, ensure_structure  # noqa: E402


BASE_URL = "http://h5.2025eyp.com"
USER_AGENT = (
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/123.0.0.0 Safari/537.36"
)

# æŽ¢æµ‹ä¸Žç´¢å¼•ç»´æŠ¤çš„æ ¸å¿ƒå‚æ•°
PROBE_MAX_FETCHES = 80
PROBE_CONSECUTIVE_MISS = 25
MISSING_BUCKET_LIMIT = 800
PROBE_HISTORY_LIMIT = 20


@dataclass
class Article:
    article_id: int
    title: str
    category: str
    date: Optional[str]
    brief: Optional[str]
    markdown: str
    source_url: str


def sanitize_filename(name: str, max_len: int = 160) -> str:
    name = re.sub(r"[\\/*?:\"<>|]", "", name).strip()
    name = re.sub(r"\s+", " ", name)
    if len(name) > max_len:
        name = name[:max_len].rstrip()
    return name or "article"


def extract_date(text: Optional[str]) -> Optional[str]:
    if not text:
        return None
    match = re.search(r"(\d{4}[.\-/]\d{1,2}[.\-/]\d{1,2})", text)
    if not match:
        return None
    y, m, d = re.split(r"[.\-/]", match.group(1))
    return f"{int(y):04d}.{int(m):02d}.{int(d):02d}"


def normalize_html(html: str) -> str:
    if not html:
        return html
    soup = BeautifulSoup(html, "html.parser")

    for img in soup.find_all("img"):
        src = img.get("src") or img.get("data-src")
        if src:
            img["src"] = urljoin(BASE_URL, src)
        if img.has_attr("data-src"):
            del img["data-src"]

    for anchor in soup.find_all("a"):
        href = anchor.get("href")
        if href:
            anchor["href"] = urljoin(BASE_URL, href)

    return str(soup)


def html_to_markdown(html: str) -> str:
    if not html:
        return ""

    soup = BeautifulSoup(html, "html.parser")

    for br in soup.find_all(["br", "hr"]):
        br.replace_with("\n")

    def convert(node) -> str:
        if isinstance(node, str):
            return node
        name = node.name.lower() if node.name else ""

        if name in {"h1", "h2", "h3", "h4", "h5", "h6"}:
            level = int(name[1])
            prefix = "#" * level
            inner = "".join(convert(child) for child in node.children).strip()
            return f"\n{prefix} {inner}\n\n"

        if name == "p":
            inner = "".join(convert(child) for child in node.children).strip()
            return f"{inner}\n\n" if inner else ""

        if name in {"ul", "ol"}:
            items = []
            for li in node.find_all("li", recursive=False):
                content = "".join(convert(child) for child in li.children).strip()
                if content:
                    items.append(f"- {content}")
            return ("\n".join(items) + "\n\n") if items else ""

        if name == "li":
            inner = "".join(convert(child) for child in node.children).strip()
            return f"- {inner}\n"

        if name in {"strong", "b"}:
            inner = "".join(convert(child) for child in node.children)
            return f"**{inner}**"

        if name in {"em", "i"}:
            inner = "".join(convert(child) for child in node.children)
            return f"*{inner}*"

        if name == "a":
            text = "".join(convert(child) for child in node.children) or node.get("href", "")
            href = node.get("href", "")
            return f"[{text}]({href})" if href else text

        if name == "img":
            alt = node.get("alt", "")
            src = node.get("src", "")
            return f"![{alt}]({src})" if src else ""

        # è¡¨æ ¼è½¬æ¢
        if name == "table":
            return convert_table(node)

        inner = "".join(convert(child) for child in node.children)
        return inner

    def convert_table(table_node) -> str:
        """å°† HTML è¡¨æ ¼è½¬æ¢ä¸º Markdown è¡¨æ ¼"""
        rows = []
        
        # æå–è¡¨å¤´ (thead > tr > th æˆ–ç¬¬ä¸€è¡Œçš„ th)
        thead = table_node.find("thead")
        headers = []
        first_row_is_header = False
        
        if thead:
            header_row = thead.find("tr")
            if header_row:
                headers = [
                    "".join(convert(child) for child in th.children).strip()
                    for th in header_row.find_all(["th", "td"])
                ]
        
        # å¦‚æžœæ²¡æœ‰ theadï¼Œæ£€æŸ¥ç¬¬ä¸€è¡Œæ˜¯å¦å…¨æ˜¯ th
        if not headers:
            first_row = table_node.find("tr")
            if first_row:
                ths = first_row.find_all("th")
                if ths:
                    headers = [
                        "".join(convert(child) for child in th.children).strip()
                        for th in ths
                    ]
                    first_row_is_header = True
        
        # æå–æ•°æ®è¡Œ (tbody > tr > td æˆ–æ‰€æœ‰ tr)
        tbody = table_node.find("tbody")
        data_rows = []
        
        if tbody:
            for tr in tbody.find_all("tr"):
                cells = [
                    "".join(convert(child) for child in td.children).strip()
                    for td in tr.find_all(["td", "th"])
                ]
                if cells:
                    data_rows.append(cells)
        else:
            # æ²¡æœ‰ tbodyï¼ŒéåŽ†æ‰€æœ‰ trï¼ˆè·³è¿‡å·²å¤„ç†çš„è¡¨å¤´ï¼‰
            all_trs = table_node.find_all("tr")
            start_idx = 1 if first_row_is_header else 0
            for tr in all_trs[start_idx:]:
                cells = [
                    "".join(convert(child) for child in td.children).strip()
                    for td in tr.find_all(["td", "th"])
                ]
                if cells:
                    # è·³è¿‡é‡å¤çš„è¡¨å¤´è¡Œï¼ˆå†…å®¹ä¸Ž headers å®Œå…¨ç›¸åŒï¼‰
                    if headers and cells == headers:
                        continue
                    data_rows.append(cells)
        
        # å¦‚æžœæ—¢æ²¡æœ‰è¡¨å¤´ä¹Ÿæ²¡æœ‰æ•°æ®ï¼Œè¿”å›žç©º
        if not headers and not data_rows:
            return ""
        
        # å¦‚æžœæ²¡æœ‰è¡¨å¤´ï¼Œä½¿ç”¨ç¬¬ä¸€è¡Œä½œä¸ºè¡¨å¤´
        if not headers and data_rows:
            headers = data_rows[0]
            data_rows = data_rows[1:]
        
        # æž„å»º Markdown è¡¨æ ¼
        if not headers:
            return ""
        
        # ç¡®ä¿æ‰€æœ‰è¡Œçš„åˆ—æ•°ä¸€è‡´
        col_count = len(headers)
        
        # è¡¨å¤´
        header_line = "| " + " | ".join(headers) + " |"
        separator = "|" + "|".join([" --- " for _ in range(col_count)]) + "|"
        
        # æ•°æ®è¡Œ
        data_lines = []
        for row in data_rows:
            # è¡¥é½æˆ–æˆªæ–­åˆ—æ•°
            row = (row + [""] * col_count)[:col_count]
            data_lines.append("| " + " | ".join(row) + " |")
        
        # ç»„åˆè¡¨æ ¼
        table_md = "\n" + header_line + "\n" + separator + "\n"
        if data_lines:
            table_md += "\n".join(data_lines) + "\n"
        table_md += "\n"
        
        return table_md

    markdown = "".join(convert(child) for child in soup.body.children) if soup.body else convert(soup)
    markdown = re.sub(r"\n{3,}", "\n\n", markdown).strip()
    return markdown + "\n"


def detect_category(title: str, preview: str, explicit: Optional[str] = None) -> str:
    if explicit and explicit in ARTICLE_CATEGORIES:
        return explicit

    text = "\n".join(filter(None, [explicit or "", title or "", preview or ""]))
    if re.search(r"å®è§‚|æ”¿ç­–|ç»æµŽ|å¤§åŠ¿|å¤®è¡Œ", text):
        return "å®è§‚åˆ†æž"
    if re.search(r"è¡Œä¸š|äº§ä¸š|æ¿å—|èµ›é“|å…¬å¸|åˆ†æž|ä¸“é¢˜|ç ”ç©¶", text):
        return "è¡Œä¸šåˆ†æž"
    return "å…¨éƒ¨ç ”æŠ¥"


def build_filename(article: Article) -> str:
    prefix = f"{article.date}-" if article.date else ""
    return sanitize_filename(f"{prefix}{article.title}") + ".md"


def ensure_index_defaults(data: Optional[Dict[str, Any]]) -> Dict[str, Any]:
    if not isinstance(data, dict):
        data = {}
    data.setdefault("saved_ids", [])
    data.setdefault("downloaded_ids", [])  # æ–°å¢žï¼šè¿½è¸ªå·²ä¸‹è½½çš„æ–‡ç« 
    data.setdefault("missing_ids", [])
    data.setdefault("pending_ids", [])
    data.setdefault("last_probed_id", 0)
    data.setdefault("next_probe_id", 1)
    data.setdefault("probe_history", [])
    return data


def read_index() -> Dict[str, Any]:
    if INDEX_FILE.exists():
        try:
            data = json.loads(INDEX_FILE.read_text(encoding="utf-8"))
            return ensure_index_defaults(data)
        except json.JSONDecodeError:
            pass
    return ensure_index_defaults({})


def write_index(data: Dict[str, Any]) -> None:
    # æ¸…ç†è°ƒè¯•ç”¨çš„ä¸´æ—¶å­—æ®µ
    dump_ready = {k: v for k, v in data.items() if not str(k).startswith("_")}
    INDEX_FILE.write_text(
        json.dumps(dump_ready, ensure_ascii=False, indent=2),
        encoding="utf-8",
    )


def add_saved_id(article_id: int, index: Dict[str, Any]) -> None:
    saved = set(int(i) for i in index.get("saved_ids", []))
    saved.add(int(article_id))
    index["saved_ids"] = sorted(saved)


def article_already_saved(article_id: int, index: Dict[str, Any]) -> bool:
    return int(article_id) in set(int(i) for i in index.get("saved_ids", []))


def article_downloaded(article_id: int, index: Dict[str, Any]) -> bool:
    """æ£€æŸ¥æ–‡ç« æ˜¯å¦å·²å®Œæˆä¸‹è½½"""
    return int(article_id) in set(int(i) for i in index.get("downloaded_ids", []))


def add_downloaded_id(article_id: int, index: Dict[str, Any]) -> None:
    """æ ‡è®°æ–‡ç« å·²æˆåŠŸä¸‹è½½"""
    downloaded = set(int(i) for i in index.get("downloaded_ids", []))
    downloaded.add(int(article_id))
    index["downloaded_ids"] = sorted(downloaded)


def record_missing_id(article_id: int, index: Dict[str, Any]) -> None:
    missing = set(int(i) for i in index.get("missing_ids", []))
    missing.add(int(article_id))
    index["missing_ids"] = sorted(missing)[-MISSING_BUCKET_LIMIT:]


def clear_missing_id(article_id: int, index: Dict[str, Any]) -> None:
    missing = set(int(i) for i in index.get("missing_ids", []))
    if int(article_id) in missing:
        missing.discard(int(article_id))
        index["missing_ids"] = sorted(missing)


def update_probe_history(index: Dict[str, Any], start_id: int, stop_id: int, found_id: int) -> None:
    history = list(index.get("probe_history", []))
    history.append(
        {
            "start": int(start_id),
            "stop": int(stop_id),
            "found": int(found_id),
            "ts": int(time.time()),
        }
    )
    index["probe_history"] = history[-PROBE_HISTORY_LIMIT:]


def resolve_probe_start(index: Dict[str, Any]) -> int:
    """[å·²å¼ƒç”¨] ä»…ç”¨äºŽæ‰‹åŠ¨æ¨¡å¼å…¼å®¹"""
    saved_ids = list(index.get("saved_ids", []))
    max_saved = max(saved_ids) if saved_ids else 0
    next_cursor = int(index.get("next_probe_id", 1))
    last_probed = int(index.get("last_probed_id", 0))
    return max(1, max_saved, next_cursor, last_probed)


# ==================== å·²å¼ƒç”¨çš„æŽ¢æµ‹å‡½æ•° ====================
# ä»¥ä¸‹å‡½æ•°å·²ç”± pre_crawl_check.py æŽ¥ç®¡,ä¿ç•™ä»…ç”¨äºŽæ‰‹åŠ¨æ¨¡å¼
# =======================================================

def fetch_pending_articles(
    fetcher: "GatorFetcher",
    pending_ids: Sequence[int],
    index: Dict[str, Any],
) -> List[Article]:
    """[å·²å¼ƒç”¨] é‡è¯• pending åˆ—è¡¨ä¸­çš„æ–‡ç« """
    results: Dict[int, Article] = {}

    for article_id in sorted({int(i) for i in pending_ids}):
        if article_already_saved(article_id, index):
            continue

        article = fetcher.fetch(article_id)
        if article:
            clear_missing_id(article_id, index)
            results[article.article_id] = article
        else:
            record_missing_id(article_id, index)

    return list(results.values())


def probe_new_articles(
    fetcher: "GatorFetcher",
    index: Dict[str, Any],
    max_fetches: int = PROBE_MAX_FETCHES,
    max_consecutive_missing: int = PROBE_CONSECUTIVE_MISS,
) -> List[Article]:
    """
    [å·²å¼ƒç”¨] æ­¤å‡½æ•°å·²ç”± pre_crawl_check.py æŽ¥ç®¡
    
    æ–°æž¶æž„:
    - pre_crawl_check.py: è½»é‡çº§è¾¹ç•ŒæŽ¢æµ‹
    - fetch_reports.py: åªè´Ÿè´£ä¸‹è½½å·²çŸ¥è¾¹ç•Œå†…çš„æ–‡ç« 
    """
    print("\nâš ï¸  è­¦å‘Š: probe_new_articles() å·²å¼ƒç”¨")
    print("   è¯·ä½¿ç”¨ pre_crawl_check.py è¿›è¡Œè¾¹ç•ŒæŽ¢æµ‹")
    return []


def manual_scan_range(
    fetcher: "GatorFetcher",
    start_id: int,
    end_id: int,
    index: Dict[str, Any],
    max_consecutive_missing: int,
) -> List[Article]:
    """æŒ‰æŒ‡å®šåŒºé—´ä¸»åŠ¨æŠ“å–æ–‡ç« ï¼Œè¿”å›žéœ€è¦ä¿å­˜çš„æ–‡ç« åˆ—è¡¨"""
    new_articles: Dict[int, Article] = {}
    saved = set(int(i) for i in index.get("saved_ids", []))

    miss_streak = 0
    last_found = int(index.get("last_probed_id", 0))
    probed_ids: List[int] = []

    for article_id in range(start_id, end_id + 1):
        probed_ids.append(article_id)

        if article_already_saved(article_id, index):
            continue

        article = fetcher.fetch(article_id)
        if article:
            new_articles[article_id] = article
            clear_missing_id(article_id, index)
            miss_streak = 0
            last_found = max(last_found, article_id)
        else:
            record_missing_id(article_id, index)
            miss_streak += 1
            if miss_streak >= max_consecutive_missing:
                print("è¿žç»­ç¼ºå¤±è¾¾åˆ°é˜ˆå€¼ï¼Œæå‰ç»“æŸæ‰«æã€‚")
                break

    stop_id = probed_ids[-1] if probed_ids else start_id - 1
    index["next_probe_id"] = max(index.get("next_probe_id", 1), stop_id + 1)
    index["last_probed_id"] = max(index.get("last_probed_id", 0), last_found)
    update_probe_history(index, start_id, stop_id, index["last_probed_id"])
    index["_last_probe_ids"] = probed_ids
    pending = set(int(i) for i in index.get("pending_ids", []))
    pending.update(
        article_id
        for article_id in new_articles
        if article_id not in saved
    )
    index["pending_ids"] = sorted(pending)
    write_index(index)

    return list(new_articles.values())


def save_article(article: Article) -> None:
    filename = build_filename(article)
    lines = [
        f"# {article.title}",
        "",
        f"- åˆ†ç±»: {article.category}",
        f"- æ—¥æœŸ: {article.date or 'æœªçŸ¥'}",
        f"- æ–‡ç« ID: {article.article_id}",
        f"- æ¥æº: {article.source_url}",
        "",
        "---",
        "",
    ]

    if article.brief:
        lines.append(f"> {article.brief.strip()}")
        lines.append("")

    lines.append(article.markdown.rstrip())
    content = "\n".join(lines).rstrip() + "\n"

    # ä¿å­˜åˆ°å…¨éƒ¨ç ”æŠ¥
    ARTICLE_CATEGORIES["å…¨éƒ¨ç ”æŠ¥"].mkdir(parents=True, exist_ok=True)
    target_all = ARTICLE_CATEGORIES["å…¨éƒ¨ç ”æŠ¥"] / filename
    target_all.write_text(content, encoding="utf-8")

    # åˆ†ç±»ä¿å­˜ï¼ˆå®è§‚/è¡Œä¸šï¼‰
    if article.category in ARTICLE_CATEGORIES and article.category != "å…¨éƒ¨ç ”æŠ¥":
        ARTICLE_CATEGORIES[article.category].mkdir(parents=True, exist_ok=True)
        ARTICLE_CATEGORIES[article.category].joinpath(filename).write_text(
            content, encoding="utf-8"
        )
    return target_all


def download_articles(
    articles: Sequence[Article],
    index: Dict[str, Any],
    sleep_seconds: float = 0.0,
) -> Tuple[int, int, int]:
    """ä¿å­˜æ–‡ç« å¹¶ç»´æŠ¤ç´¢å¼•ï¼Œè¿”å›ž (æ–°å¢ž, è·³è¿‡, å¤±è´¥)"""
    saved_ids = set(int(i) for i in index.get("saved_ids", []))
    downloaded_ids = set(int(i) for i in index.get("downloaded_ids", []))
    pending_ids = set(int(i) for i in index.get("pending_ids", []))

    success = 0
    skipped = 0
    failed = 0

    for article in sorted(articles, key=lambda a: a.article_id):
        article_id = int(article.article_id)

        # æ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½ï¼ˆè€Œéžä»…æ£€æŸ¥æ˜¯å¦å·²ä¿å­˜ï¼‰
        if article_id in downloaded_ids:
            pending_ids.discard(article_id)
            skipped += 1
            continue

        try:
            save_article(article)
        except Exception as exc:  # noqa: BLE001 - è®°å½•å¤±è´¥åŽç»§ç»­
            print(f"[{article_id}] ä¿å­˜å¤±è´¥: {exc}")
            record_missing_id(article_id, index)
            pending_ids.add(article_id)
            failed += 1
            continue

        add_saved_id(article_id, index)
        add_downloaded_id(article_id, index)  # æ ‡è®°ä¸ºå·²ä¸‹è½½
        clear_missing_id(article_id, index)
        pending_ids.discard(article_id)
        saved_ids.add(article_id)
        downloaded_ids.add(article_id)
        success += 1

        index["pending_ids"] = sorted(pending_ids)
        index["downloaded_ids"] = sorted(downloaded_ids)
        write_index(index)

        if sleep_seconds > 0:
            time.sleep(sleep_seconds)

    index["pending_ids"] = sorted(pending_ids)
    index["saved_ids"] = sorted(saved_ids)
    index["downloaded_ids"] = sorted(downloaded_ids)
    write_index(index)

    return success, skipped, failed


class GatorFetcher:
    def __init__(self, headless: bool = True, timeout: int = 20, save_html: bool = False):
        self.headless = headless
        self.timeout = timeout
        self.save_html = save_html
        self.driver: Optional[webdriver.Chrome] = None

    def __enter__(self) -> "GatorFetcher":
        ensure_structure()
        options = Options()
        if self.headless:
            options.add_argument("--headless=new")
        options.add_argument("--disable-gpu")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--window-size=1920,1080")
        options.add_argument(f"--user-agent={USER_AGENT}")
        options.add_argument("--disable-blink-features=AutomationControlled")

        try:
            self.driver = webdriver.Chrome(options=options)
        except WebDriverException as exc:  # noqa: BLE001
            raise RuntimeError(f"æ— æ³•å¯åŠ¨ Chrome WebDriver: {exc}") from exc

        self.driver.set_page_load_timeout(self.timeout)
        return self

    def __exit__(self, exc_type, exc, tb) -> None:
        if self.driver:
            try:
                self.driver.quit()
            except Exception:
                pass

    def _wait_for_content(self) -> None:
        assert self.driver is not None
        try:
            WebDriverWait(self.driver, self.timeout).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "body"))
            )
            WebDriverWait(self.driver, self.timeout).until(self._has_meaningful_text)
        except TimeoutException:
            pass

    @staticmethod
    def _has_meaningful_text(driver) -> bool:
        try:
            article = driver.find_element(By.CSS_SELECTOR, ".article")
            if article.text and len(article.text.strip()) > 40:
                return True
        except NoSuchElementException:
            pass
        try:
            body = driver.find_element(By.TAG_NAME, "body")
            return len(body.text.strip()) > 200
        except NoSuchElementException:
            return False

    def fetch(self, article_id: int) -> Optional[Article]:
        """
        èŽ·å–å•ç¯‡æ–‡ç« çš„å®Œæ•´å†…å®¹
        
        âš ï¸ å…³é”®ï¼šæ–‡ç« å­˜åœ¨æ€§åˆ¤æ–­ (2025-11-01 éªŒè¯)
        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        1. è¿™æ˜¯ SPA (å•é¡µåº”ç”¨)ï¼Œéœ€è¦ç­‰å¾… JS åŠ¨æ€åŠ è½½å†…å®¹
        2. æ–‡ç« ä¸å­˜åœ¨æ—¶ï¼Œé¡µé¢åªæ˜¾ç¤ºå…è´£å£°æ˜Ž:
           "é³„é±¼æ´¾å£°æ˜Žï¼šæ–‡ç« å†…å®¹ä»…ä¾›å‚è€ƒï¼Œä¸æž„æˆæŠ•èµ„å»ºè®®ã€‚æŠ•èµ„è€…æ®æ­¤æ“ä½œï¼Œé£Žé™©è‡ªæ‹…ã€‚"
        3. æ–‡ç« å­˜åœ¨æ—¶ï¼Œé¡µé¢ä¼šæ˜¾ç¤ºæ ‡é¢˜ã€æ—¥æœŸã€æ­£æ–‡ç­‰å®Œæ•´å†…å®¹ (é€šå¸¸ > 200 å­—ç¬¦)
        4. å…è´£å£°æ˜Žæ˜¯åˆ¤æ–­æ–‡ç« ä¸å­˜åœ¨çš„å…³é”®æ ‡è¯†ï¼
        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        """
        assert self.driver is not None
        url = f"{BASE_URL}/articles/{article_id}"
        try:
            self.driver.get(url)
            self._wait_for_content()
        except TimeoutException:
            print(f"[{article_id}] é¡µé¢åŠ è½½è¶…æ—¶")
            return None
        except WebDriverException as exc:  # noqa: BLE001
            print(f"[{article_id}] é¡µé¢åŠ è½½å¤±è´¥: {exc}")
            return None

        page_source = self.driver.page_source

        if self.save_html:
            RAW_HTML_DIR.joinpath(f"article_{article_id}.html").write_text(
                page_source, encoding="utf-8"
            )

        # æ£€æŸ¥æ–‡ç« æ˜¯å¦å­˜åœ¨ï¼šå…³é”®åˆ¤æ–­æ ‡è¯†
        if "æ‰¾ä¸åˆ°é¡µé¢" in page_source or "404" in page_source:
            return None

        soup = BeautifulSoup(page_source, "html.parser")

        def first_text(selectors: Iterable[str]) -> str:
            for selector in selectors:
                element = soup.select_one(selector)
                if element and element.get_text(strip=True):
                    return element.get_text(strip=True)
            return ""

        def first_html(selectors: Iterable[str]) -> str:
            for selector in selectors:
                element = soup.select_one(selector)
                if element and element.decode_contents():
                    return element.decode_contents()
            return ""

        title = first_text([".article .title", ".article-title", "article h1", "h1"])
        category_raw = first_text([".article .tags", ".article .category", ".article .cate"])
        date_text = first_text([".article .time", ".article .date", "time"])
        brief = first_text([".article .brief", ".article .summary"])
        content_html = first_html(
            [
                ".article .md-editor-preview",
                ".article .content",
                ".article-content",
                "article",
            ]
        )

        content_html = normalize_html(content_html)
        markdown = html_to_markdown(content_html) if content_html else ""

        if not markdown or len(markdown.strip()) < 40:
            body_text = first_text([".article", "body"])
            if not body_text or len(body_text) < 40:
                return None
            markdown = body_text.strip() + "\n"

        category = detect_category(title, markdown[:200], category_raw)
        date_fmt = extract_date(date_text) or extract_date(markdown)

        return Article(
            article_id=article_id,
            title=title or f"article_{article_id}",
            category=category,
            date=date_fmt,
            brief=brief,
            markdown=markdown,
            source_url=url,
        )


def parse_args(argv: Optional[Iterable[str]] = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="ä¸‹è½½é³„é±¼æ´¾ç ”æŠ¥ (ä»…ä¸‹è½½,ä¸æŽ¢æµ‹è¾¹ç•Œ)",
        epilog="æç¤º: è¾¹ç•ŒæŽ¢æµ‹è¯·ä½¿ç”¨ python scripts/pre_crawl_check.py"
    )
    parser.add_argument("--start-id", type=int, help="æ‰‹åŠ¨æ¨¡å¼: èµ·å§‹ ID")
    parser.add_argument("--end-id", type=int, help="æ‰‹åŠ¨æ¨¡å¼: ç»“æŸ ID")
    parser.add_argument("--batch-size", type=int, help="[å·²å¼ƒç”¨] æ”¹ç”¨ --max-requests")
    parser.add_argument("--max-requests", type=int, help="å•æ¬¡æœ€å¤šä¸‹è½½å¤šå°‘ç¯‡æ–‡ç«  (é»˜è®¤: å…¨éƒ¨)")
    parser.add_argument("--max-miss", type=int, default=25, help="æ‰‹åŠ¨æ¨¡å¼: è¿žç»­ç¼ºå¤±é˜ˆå€¼ (é»˜è®¤ 25)")
    parser.add_argument("--no-headless", action="store_true", help="æ˜¾ç¤ºæµè§ˆå™¨çª—å£ (è°ƒè¯•ç”¨)")
    parser.add_argument("--save-html", action="store_true", help="ä¿å­˜åŽŸå§‹ HTML (è°ƒè¯•ç”¨)")
    parser.add_argument("--sleep", type=float, default=1.0, help="è¯·æ±‚é—´éš”ç§’æ•° (é»˜è®¤ 1.0)")
    return parser.parse_args(argv)


def determine_range(args: argparse.Namespace, index: Dict[str, Any]) -> Tuple[int, int]:
    if args.start_id is not None and args.end_id is not None:
        return args.start_id, args.end_id
    if args.start_id is not None and args.end_id is None:
        return args.start_id, args.start_id + args.batch_size - 1

    start = resolve_probe_start(index)
    end = start + args.batch_size - 1
    return start, end


def verify_downloaded_files(index: Dict[str, Any]) -> tuple[set[int], set[int], set[int]]:
    """
    éªŒè¯ downloaded_ids å¯¹åº”çš„æ–‡ä»¶æ˜¯å¦å®žé™…å­˜åœ¨
    
    è¿”å›ž: (å®žé™…å­˜åœ¨çš„ ID, æ–‡ä»¶ä¸¢å¤±çš„ ID, é¢å¤–çš„ ID)
    
    âš ï¸ æ•°æ®ä¸€è‡´æ€§ä¿è¯ (2025-11-01):
    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    é—®é¢˜ 1: downloaded_ids è®°å½•äº† ID,ä½† MD æ–‡ä»¶å¯èƒ½è¢«æ‰‹åŠ¨åˆ é™¤
    é—®é¢˜ 2: æ‰‹åŠ¨æ·»åŠ çš„ MD æ–‡ä»¶,ä½† downloaded_ids æ²¡æœ‰è®°å½•
    è§£å†³: å¯åŠ¨æ—¶éªŒè¯æ–‡ä»¶å®žé™…å­˜åœ¨,åŒå‘åŒæ­¥ downloaded_ids
    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    """
    downloaded_ids = set(int(i) for i in index.get("downloaded_ids", []))
    
    # æ‰«ææ‰€æœ‰ MD æ–‡ä»¶,æå– article_id
    existing_ids = set()
    for category_path in ARTICLE_CATEGORIES.values():
        if not category_path.exists():
            continue
        
        for md_file in category_path.glob("*.md"):
            if md_file.name.lower() == "readme.md":
                continue
            
            # ä»Žæ–‡ä»¶å†…å®¹ä¸­æå– article_id
            try:
                content = md_file.read_text(encoding="utf-8")
                match = re.search(r"^- æ–‡ç« ID:\s*(\d+)", content, re.MULTILINE)
                if match:
                    existing_ids.add(int(match.group(1)))
            except Exception:
                continue
    
    # è®¡ç®—å·®å¼‚
    missing_files = downloaded_ids - existing_ids  # JSON æœ‰ä½†æ–‡ä»¶ä¸å­˜åœ¨
    extra_files = existing_ids - downloaded_ids    # æ–‡ä»¶å­˜åœ¨ä½† JSON æ²¡è®°å½•
    
    return existing_ids, missing_files, extra_files


def run_incremental_mode(args: argparse.Namespace, index: Dict[str, Any]) -> int:
    """
    å¢žé‡ä¸‹è½½æ¨¡å¼ (ä¸å†æŽ¢æµ‹,åªä¸‹è½½)
    
    âš ï¸ èŒè´£åˆ†ç¦» (2025-11-01 é‡æž„):
    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    1. pre_crawl_check.py è´Ÿè´£è¾¹ç•ŒæŽ¢æµ‹,å†™å…¥ last_probed_id
    2. fetch_reports.py åªè´Ÿè´£ä¸‹è½½,ä»Ž index.json è¯»å–è¾¹ç•Œ
    3. ä¸‹è½½èŒƒå›´: ä»Ž 1 åˆ° last_probed_id (è·³è¿‡å·²ä¸‹è½½çš„)
    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    """
    # ðŸ†• ç¬¬ä¸€æ­¥: éªŒè¯æ–‡ä»¶å®žé™…å­˜åœ¨å¹¶åŒæ­¥
    print("ðŸ” éªŒè¯å·²ä¸‹è½½æ–‡ä»¶...")
    existing_ids, missing_files, extra_files = verify_downloaded_files(index)
    
    sync_needed = False
    
    if missing_files:
        print(f"âš ï¸  å‘çŽ° {len(missing_files)} ç¯‡æ–‡ä»¶ä¸¢å¤± (JSON æœ‰è®°å½•ä½†æ–‡ä»¶ä¸å­˜åœ¨)")
        print(f"   ä¸¢å¤± ID: {sorted(list(missing_files))[:20]}{'...' if len(missing_files) > 20 else ''}")
        sync_needed = True
    
    if extra_files:
        print(f"ðŸ“¥ å‘çŽ° {len(extra_files)} ç¯‡é¢å¤–æ–‡ä»¶ (æ–‡ä»¶å­˜åœ¨ä½† JSON æœªè®°å½•)")
        print(f"   é¢å¤– ID: {sorted(list(extra_files))[:20]}{'...' if len(extra_files) > 20 else ''}")
        sync_needed = True
    
    if sync_needed:
        # åŒå‘åŒæ­¥: ä»¥å®žé™…æ–‡ä»¶ä¸ºå‡†
        index["downloaded_ids"] = sorted(list(existing_ids))
        index["saved_ids"] = sorted(list(set(index.get("saved_ids", [])) | existing_ids))
        write_index(index)
        print(f"âœ… å·²åŒæ­¥ downloaded_ids: {len(index['downloaded_ids'])} ç¯‡")
    else:
        print(f"âœ… æ–‡ä»¶éªŒè¯é€šè¿‡: {len(existing_ids)} ç¯‡")
    
    downloaded_ids = existing_ids  # ä½¿ç”¨å®žé™…å­˜åœ¨çš„ ID
    boundary = int(index.get("last_probed_id", 0))
    
    print("ðŸ“Š å½“å‰ç´¢å¼•çŠ¶æ€:")
    print(f"   å·²ä¸‹è½½: {len(downloaded_ids)} ç¯‡")
    print(f"   æŽ¢æµ‹è¾¹ç•Œ: ID {boundary} (ç”± pre_crawl_check.py å†™å…¥)")
    
    if boundary == 0:
        print("\nâŒ é”™è¯¯: è¾¹ç•ŒæœªæŽ¢æµ‹ (last_probed_id = 0)")
        print("   è¯·å…ˆè¿è¡Œ: python scripts/pre_crawl_check.py")
        return 1
    
    # è®¡ç®—éœ€è¦ä¸‹è½½çš„ ID åˆ—è¡¨ (1 åˆ° boundary, è·³è¿‡å·²ä¸‹è½½)
    all_ids = set(range(1, boundary + 1))
    known_missing = set(int(i) for i in index.get("missing_ids", []))
    to_download = sorted(all_ids - downloaded_ids - known_missing)
    
    if not to_download:
        print("\nâœ… æ‰€æœ‰æ–‡ç« å·²ä¸‹è½½å®Œæˆ!")
        print(f"   è¾¹ç•Œå†…æ–‡ç« : {boundary} ç¯‡")
        print(f"   å·²ä¸‹è½½: {len(downloaded_ids)} ç¯‡")
        print(f"   å·²çŸ¥ç¼ºå¤±: {len(known_missing)} ç¯‡")
        return 0
    
    print(f"\nðŸ“¥ å¾…ä¸‹è½½æ–‡ç« : {len(to_download)} ç¯‡")
    print(f"   ID èŒƒå›´: {to_download[0]} - {to_download[-1]}")
    print(f"   å‰ 20 ä¸ª: {to_download[:20]}")
    
    # é™åˆ¶å•æ¬¡ä¸‹è½½æ•°é‡
    max_download = args.max_requests if args.max_requests else len(to_download)
    if args.batch_size:  # å…¼å®¹æ—§å‚æ•°
        print(f"âš ï¸  --batch-size å·²å¼ƒç”¨ï¼Œè¯·ä½¿ç”¨ --max-requests")
        max_download = args.batch_size
    
    to_download_batch = to_download[:max_download]
    print(f"\nðŸš€ æœ¬æ¬¡ä¸‹è½½: {len(to_download_batch)} ç¯‡ (å‰©ä½™ {len(to_download) - len(to_download_batch)} ç¯‡)")
    
    # å¼€å§‹ä¸‹è½½
    articles_to_save: List[Article] = []
    success_count = 0
    skip_count = 0
    fail_count = 0
    
    with GatorFetcher(headless=not args.no_headless, save_html=args.save_html) as fetcher:
        for idx, article_id in enumerate(to_download_batch, 1):
            print(f"\nðŸ“„ [{idx}/{len(to_download_batch)}] ä¸‹è½½ ID {article_id}...", end=" ")
            
            article = fetcher.fetch(article_id)
            if article:
                articles_to_save.append(article)
                success_count += 1
                print(f"âœ… {article.title[:40]}...")
            else:
                record_missing_id(article_id, index)
                fail_count += 1
                print(f"âŒ æœªæ‰¾åˆ°")
            
            if args.sleep > 0:
                time.sleep(args.sleep)
        
        # æ‰¹é‡ä¿å­˜
        if articles_to_save:
            print(f"\nðŸ’¾ å¼€å§‹ä¿å­˜ {len(articles_to_save)} ç¯‡æ–‡ç« ...")
            saved, skipped, failed = download_articles(
                articles_to_save,
                index,
                sleep_seconds=0,  # å·²ç»åœ¨ä¸‹è½½æ—¶sleepäº†
            )
        else:
            print("\nâš ï¸  æœ¬æ¬¡æœªæˆåŠŸä¸‹è½½ä»»ä½•æ–‡ç« ")
            saved = skipped = failed = 0
    
    # æœ€ç»ˆç»Ÿè®¡
    final_downloaded = len(index.get("downloaded_ids", []))
    final_missing = len(index.get("missing_ids", []))
    remaining = boundary - final_downloaded - final_missing
    
    print("\n" + "=" * 60)
    print("âœ… ä¸‹è½½å®Œæˆ")
    print("=" * 60)
    print(f"æœ¬æ¬¡ç»“æžœ:")
    print(f"   æˆåŠŸä¸‹è½½: {saved} ç¯‡")
    print(f"   è·³è¿‡: {skipped} ç¯‡")
    print(f"   å¤±è´¥: {failed} ç¯‡")
    print(f"\næ€»ä½“è¿›åº¦:")
    print(f"   è¾¹ç•Œ: ID {boundary}")
    print(f"   å·²ä¸‹è½½: {final_downloaded} ç¯‡")
    print(f"   å·²çŸ¥ç¼ºå¤±: {final_missing} ç¯‡")
    print(f"   å‰©ä½™å¾…ä¸‹è½½: {remaining} ç¯‡")
    
    if remaining > 0:
        print(f"\nðŸ’¡ æç¤º: å†æ¬¡è¿è¡Œæœ¬è„šæœ¬å¯ç»§ç»­ä¸‹è½½å‰©ä½™ {remaining} ç¯‡æ–‡ç« ")
    else:
        print(f"\nðŸŽ‰ æ­å–œ! è¾¹ç•Œå†…æ‰€æœ‰æ–‡ç« å·²ä¸‹è½½å®Œæˆ!")
    
    return 0


def run_manual_range_mode(args: argparse.Namespace, index: Dict[str, Any]) -> int:
    start_id, end_id = determine_range(args, index)
    print(f"ðŸ”§ æ‰‹åŠ¨æ¨¡å¼ï¼šæ‰«æ ID {start_id} ~ {end_id}")

    with GatorFetcher(headless=not args.no_headless, save_html=args.save_html) as fetcher:
        articles = manual_scan_range(
            fetcher,
            start_id,
            end_id,
            index,
            max_consecutive_missing=max(1, args.max_miss),
        )

    if articles:
        print(f"ðŸŽ¯ éœ€è¦ä¿å­˜ {len(articles)} ç¯‡æ–‡ç« ...")
        success, skipped, failed = download_articles(
            articles,
            index,
            sleep_seconds=max(0.0, args.sleep),
        )
    else:
        print("ðŸŽ¯ æ‰‹åŠ¨æ‰«ææœªå‘çŽ°å¯ä¿å­˜çš„æ–°æ–‡ç« ã€‚")
        success = skipped = failed = 0

    final_total = len(index.get("saved_ids", []))
    print("\nâœ… æ‰‹åŠ¨æ‰«æå®Œæˆ")
    print(f"   æ–°å¢ž: {success} ç¯‡ï¼Œè·³è¿‡: {skipped} ç¯‡ï¼Œå¤±è´¥: {failed} ç¯‡")
    print(f"   å½“å‰æ€»é‡: {final_total} ç¯‡")
    print(f"   æœ€æ–°æŽ¢æµ‹ ID: {index.get('last_probed_id', 0)}")
    print(f"   ä¸‹ä¸€æ¬¡æŽ¢æµ‹å°†ä»Ž ID {index.get('next_probe_id', 1)} å¼€å§‹")
    return 0


def main(argv: Optional[Iterable[str]] = None) -> int:
    args = parse_args(argv)
    ensure_structure()
    index = read_index()

    manual_mode = args.start_id is not None or args.end_id is not None
    if manual_mode:
        return run_manual_range_mode(args, index)
    return run_incremental_mode(args, index)


if __name__ == "__main__":
    sys.exit(main())
