# 2025.8.20AI行业观察

- 分类: 行业分析
- 日期: 2025.08.20
- 文章ID: 481
- 来源: http://h5.2025eyp.com/articles/481

---

> 从英伟达的视角看算力互连板块成长性 ——Scale Up 网络的 “Scaling Law” 存在吗？

## **核心观点**

研究认为 Scale Up 网络存在 Scaling Law，Scale Up 柜间第二层网络会逐渐出现，光 + AEC 连接与芯片有 1:9 的配比需求，交换机与芯片有 4:1 的配比需求，相较 Scale Out 网络均倍增：

1. 英伟达持续扩大 Scale Up 规模：英伟达通过提升单卡带宽和扩大超节点规模两条路径持续扩大 Scale Up 网络规模。NVLink 不断迭代，NVLink 5.0 单卡带宽达 7200Gb/s；Scale Up 超节点规模从 H100 NVL8 到 GH200 再到 GB200 等不断扩大，NVL72 等机柜方案会作为最小节点，在柜与柜之间进一步拼出更大的 Scale Up 超节点，届时需要光连接等进行通信。

2. 需要 Scale Up 网络的原因：“内存墙” 问题和 AI 计算范式演进推动 Scale Up 网络升级。“内存墙” 方面，单一大模型的参数量与单卡显存的差距、单卡算力与单卡显存间的差距均逐代放大，可通过 Scale Up 将显存池化。计算范式方面，为提升计算效率，在采用数据并行、流水线并行的同时也采用张量并行与专家并行，后两者对通信频次、容量的要求跨越数量级。

3. 需要更大的 Scale Up 网络的原因：涉及 TCO、用户体验、模型能力拓展。随着单用户每秒消耗的 Token 数（TPS）提高，现有服务器单卡性能会逐渐坍缩，在用户体验提升、模型能力拓展的趋势下，单用户 TPS 必然增长，采用更大规模的 Scale Out 能提高单卡有效性能，TCO 也更具经济性。Scale Up 规模与预期单用户 TPS、单卡实际性能间存在 Scaling Law，前者会随后者非线性增长。

4. 组建更大的 Scale Up 网络的方式：网络结构层面，在柜间搭建第二层 Scale Up 交换机；端口连接层面，光与 AEC 有望在第二层网络中并存，按照最新的 NVLink 与 IB 标准测算，1 颗 GPU 需要 9 个额外的等效 1.6T 连接，为 Scale Out 网络的 3-4.5 倍，每 4 颗 GPU 需要额外 1 台交换机，为 Scale Out 网络的 7.5-12 倍。

5. 算力互连需求的乘数效应：资本开支结构优化，算力芯片增长速度高于资本开支增速；单芯片带宽提升，算力互连需求增速高于芯片需求增速。其中，芯片需求 = CapE× 算力芯片投资在 CapEx 占比 × 芯片投资性价比；算力互连需求 = 芯片需求 × 单芯片带宽及 Scale Up 层数。

## **英伟达持续扩大 Scale Up 规模**

1. 英伟达持续尝试扩大 Scale Up 规模：英伟达从单卡带宽与超节点规模两个路径升级 Scale Up。NVLink 跟随每一代 GPU 架构升级，最新用于 B 系列 GPU 的 NVLink 5.0 可支持单卡 7.2Tb 的带宽，相较用于 H100 的 NVLink 4.0 带宽翻倍；Scale Up 超节点规模在 H100 之后经历了 GH200、GB200 等方案，从 NVL8 拓展至 NVL72 甚至更高。

2. H100 NVL8 到 GH200 NVL256：英伟达在 2023 年基于 H200 发布了 GH200 NVL256 超节点，由 32 个计算 Chassis 组成，每个 Chassis 由 8 张 GH200 组成。Chassis 内 8 张 GH200 通过 L1 NVSwitch 连接，32 个 Chassis 间通过 L2 NVSwitch 连接。L2 NVSwitch 通过光连接，每张 GPU 配套 8 个 800G 光模块，大约每 7 张 GPU 对应一台 L2 NVSwitch。但单张 GPU 配套 Scale Up 的通信硬件成本较高，且训练、推理性能提升不明显，未大范围推广，后续推出成本更低的 GH200 NVL32。

3. GB 与 VR 机柜：机柜方案延续了英伟达在 GH200 NVL256 上的思路，除提升 NVLink 带宽外，还提高 Scale Up 超节点的规模，升级为机柜方案是为增加 GPU 密度，节省物理空间并缩小 GPU 间连接距离，使用成本更低的 PCB、铜连接。铜连接、PCB、液冷、电源等都随着 GPU 密度提高实现单张 GPU 对应价值量的跃升。机柜方案实现的 NVL72、NVL144 等 Scale Up 可提高训练、推理效率，但不是上限，后续会作为最小的 Scale Up 节点，在柜与柜之间进一步拼出更大的 Scale Up 超节点，届时需要光连接等进行通信。

## **为什么需要 Scale Up 网络**

1. Scale Up 与 Scale out 的特点与作用：若干超节点组成集群。Scale Out 网络实现集群内所有 GPU 卡互联，亮点是网络内连接 GPU 数量大，与传统数据中心网络类似；Scale Up 网络实现超节点内所有 GPU 卡互联，亮点是网络内单卡通信带宽高，组网规模尚小，为 AI 算力场景下新兴的网络架构，且不仅限于柜内，柜外也可进行。

2. “内存墙” 问题需要 Scale Up 网络：训推计算的 “内存墙” 催生出通过 Scale Up 网络将显存池化的需求。单一大模型的参数量与单卡显存的差距、单卡算力与单卡显存间的差距均逐代放大。除模型参数外，推理计算生成的 KV Cache 占用显存大小也可达模型的 50% 甚至以上。因此单卡运算时需从多张卡的显存读取所需参数、数据，目前产业化应用最优解是使用 Scale Up 网络将显存池化。

3. AI 训推计算范式推动 Scale Up 升级：AI 训推需要分布式并行计算，基于对计算效率的追求，并行计算方式有数据并行、流水线并行、专家并行及张量并行。其中，张量并行、专家并行是粒度更细的并行方式，能更高效利用单张芯片配套内存，明显提升计算效率。但张量并行在每一层神经网络的计算后都需要将新的计算结果收集、汇总，并将完整结果重新分发，即 Allreduce 通信，因此在训推时对通信频率、传输容量有更高要求，需要用 Scale Up 满足。

## **为什么需要更大的 Scale Up 网络**

1. Scale Up 可加速推理，且增益随推理负载提升而扩大：Scale Up 规模越大，集群算力有效利用率往往越高，且随着单用户推理负载增加，增益会越来越大。以 GB200 NVL72、B200 NVL8 的对比为例，在单用户 TPS 为 10 Tokens/s 时，GB200 NVL72 的单卡实际性能约为 B200 NVL8 的 3 倍，考虑 FP4 精度优化带来的约 1 倍提升后，Scale Up+Grace CPU 带来约 50% 的性能提升；在单用户 TPS 为 20 Tokens/s 时，GB200 NVL72 的单卡实际性能约为 B200 NVL8 的 7 倍，考虑 FP4 精度优化后，带来约 250% 的性能提升。随着单用户 TPS 增加，Scale Up 带来的单卡利用率增益会越来越大。

2. NVL72、144 不是推理 Scale Up 的上限：机柜对应的 NVL72、NVL144 等方案并不是 Scale Up 超节点的上限，机柜会像积木一样进一步拼出更大的超节点，这来自硬件 TCO、用户体验、模型能力拓展三层因素。当单用户 TPS 提高到 50 Tokens/s 时，B200 NVL8、H200 NVL8 的单卡真实性能已无实际意义，GB200 NVL72 仍有 70 Tokens/s 的单卡 TPS，但已相较最大性能缩减 50%。要继续提高单卡性能，除软件层面引入新的推理引擎，还需提升 Scale Up 规模，以及增加混合并行线路数。

3. 组建更大 Scale Up 网络的 TCO 优于堆更多 GPU：在前述单用户 TPS 达到某一数值时，GB200 NVL576 的单卡 TPS 可做到 GB200 NVL72 的两倍，且单用户 TPS 继续增长时，性能差距会进一步扩大。此时采用 NVL576 方案 TCO 更优，且单用户 TPS 继续提升后，TCO 的优势还将随着单卡性能差距持续扩大。

4. 用户体验及模型能力拓展推动单用户 TPS 增长：各能力带的 LLM 单用户 TPS 均不断提升，目前前五名主流模型的单用户 TPS 均在 200 Tokens/s 以上。用户使用模型过程中最直观、最核心的体验点是回答结果的生成速度，即单用户的 TPS，且 TPS 提升后具有实际应用意义的场景会逐渐涌现，如 AI coding。同时，模型能力从 LLM 扩展到多模态。

5. Scale Up 网络存在随用户 TPS 增长的 “Scaling Law”：随着用户体验提升、模型能力拓展，横轴单用户 TPS 将不断向右延伸，下降越慢的曲线在满足推理负载的同时能够实现更小的 TCO，而搭建更大规模的 Scale Up 网络是缓解下降最有效的方法之一。所需 Scale Up 规模的增速高于单用户 TPS 及单卡性能增速，且横纵指标成反比，可推测所需 Scale Up 网络规模与单用户 TPS、单卡实际性能及模型参数量间存在 Scaling Law，即 S=aGbUc（其中 S 为 Scale Up 超节点规模，a 为与模型参数、并行计算路数等有关的变量，G 为单张 GPU 的实际性能，b 为大于 1 的常数，U 为单用户 TPS，c 为大于 1 的常数）。基于此，随着推理时对单用户 TPS、单 GPU 实际性能要求的提高，Scale Up 规模会非线性增长。

## **怎么组建更大的 Scale Up 网络**

1. 网络架构：英伟达的机柜中加入了一层 NVSwitch，以 GB200 NVL72 为例，单颗 B200 NVLink 带宽 7.2Tb，9 个 Switch Tray 总带宽刚好与 72 颗 B200 进行无阻塞通信，若在柜内继续增加 GPU，需要同步增加配套 Switch Tray，物理空间和距离增加。因此在 GB 机柜使用铜连接，VR 机柜增加 PCB 后，柜内扩展难度增加，需要增加第二层交换机做柜间 Scale Up。对于 NVL72 而言，则需要改为 NVL36×2 以使得第一层 Switch Tray 翻倍至 18 个，以提供连接至第二层 NVSwitch 的上行带宽。

2. 连接方式：在单通道 200G 速率下，无源铜的有效距离上限在 1m 左右，基本无法满足跨柜 Scale Up 的连接需求，有源铜的有效距离上限在 3 米左右，可满足部分跨柜 Scale Up 的连接需求，光可满足所有跨柜 Scale Up 的连接距离要求。因此在第二层柜间 Scale Up 场景会有光与 AEC 并存。按照最新的 NVLink 与 IB 标准测算，第二层 Scale Up 网络中 1 颗 GPU 需要 9 个额外的等效 1.6T 连接，每 4 颗 GPU 需要额外 1 台 NVLink 5.0 交换机；两到三层 Scale Out 中 1 颗 GPU 对应 2-3 个等效 1.6T 连接，每 30-48 颗 GPU 对应一台 Quantum-X800 Q34xx 系列交换机。目前 Scale Up 与 Scale Out 并存，其最终形态是做到与 Scale Out 相近的规模后取代 Scale out，但需要考虑到在成本与物理空间维度都数倍增长的网络连接。此外，CPO、OCS 等潜在新技术在 Scale Up 中的应用会比 Scale Out 更难，这些新技术在 Scale Out 中规模化应用后，对它们在 Scale Up 中应用可能性的讨论才有实际意义。

3. Scale Up 需求凸显，产业链增速高于 Capex & 芯片增速：后续算力互连需求发展存在乘数效应，资本开支结构优化，算力芯片增长速度高于资本开支增速；单芯片带宽提升，算力互连需求增速高于芯片需求增速。其中，芯片需求 = CapE× 算力芯片投资在 CapEx 占比 × 芯片投资性价比；算力互连需求 = 芯片需求 × 单芯片带宽及 Scale Up 层数。

## **风险提示**

· 算力互连需求不及预期：若下游客户算力建设投入未达预期，或 AI 算力网络带宽规模未达预期，各客户对于网络互连产品的需求也将不及预期，相关公司业绩会受影响。

· 客户开拓与份额不及预期：如果相关公司未如预期开拓潜在客户，或在客户处份额低于预期，公司业绩将受到影响。

· 产品研发落地不及预期：如果相关公司在具有潜在应用前景的产品研发及量产应用上未达预期，将对公司业绩造成影响。

· 行业竞争加剧：如果行业竞争持续加剧，相关公司产品份额存在下降的可能。
